{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Classification \n",
    "\n",
    "We will predict the category of discussion posts in a newsgroup.\n",
    "\n",
    "**The unit of analysis is a discussion post**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random_seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>graphics</th>\n",
       "      <th>hockey</th>\n",
       "      <th>medical</th>\n",
       "      <th>newsgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have a few reprints left of chapters from my...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gnuplot, etc. make it easy to plot real valued...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article-I.D.: snoopy.1pqlhnINN8k1 References: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I am looking to add voice input capabil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently got a file describing a library of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>carl@SOL1.GPS.CALTECH.EDU (Carl J Lydick) writ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>In article &lt; 1qmlgaINNjab@hp-col.col.hp.com&gt; ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>Article-I.D.: kestrel.1993Apr16.172052.27843 R...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>In article &lt; 1qmlgaINNjab@hp-col.col.hp.com&gt; ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>I have a 42 yr old male friend, misdiagnosed a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXT  graphics  hockey  \\\n",
       "0    I have a few reprints left of chapters from my...         1       0   \n",
       "1    gnuplot, etc. make it easy to plot real valued...         1       0   \n",
       "2    Article-I.D.: snoopy.1pqlhnINN8k1 References: ...         1       0   \n",
       "3    Hello, I am looking to add voice input capabil...         1       0   \n",
       "4    I recently got a file describing a library of ...         1       0   \n",
       "..                                                 ...       ...     ...   \n",
       "592  carl@SOL1.GPS.CALTECH.EDU (Carl J Lydick) writ...         0       0   \n",
       "593  In article < 1qmlgaINNjab@hp-col.col.hp.com> ,...         0       0   \n",
       "594  Article-I.D.: kestrel.1993Apr16.172052.27843 R...         0       0   \n",
       "595  In article < 1qmlgaINNjab@hp-col.col.hp.com> ,...         0       0   \n",
       "596  I have a 42 yr old male friend, misdiagnosed a...         0       0   \n",
       "\n",
       "     medical newsgroup  \n",
       "0          0  graphics  \n",
       "1          0  graphics  \n",
       "2          0  graphics  \n",
       "3          0  graphics  \n",
       "4          0  graphics  \n",
       "..       ...       ...  \n",
       "592        1   medical  \n",
       "593        1   medical  \n",
       "594        1   medical  \n",
       "595        1   medical  \n",
       "596        1   medical  \n",
       "\n",
       "[597 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('news.csv')\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(597, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>graphics</th>\n",
       "      <th>hockey</th>\n",
       "      <th>medical</th>\n",
       "      <th>newsgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have a few reprints left of chapters from my...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gnuplot, etc. make it easy to plot real valued...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article-I.D.: snoopy.1pqlhnINN8k1 References: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I am looking to add voice input capabil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently got a file describing a library of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>graphics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  graphics  hockey  \\\n",
       "0  I have a few reprints left of chapters from my...         1       0   \n",
       "1  gnuplot, etc. make it easy to plot real valued...         1       0   \n",
       "2  Article-I.D.: snoopy.1pqlhnINN8k1 References: ...         1       0   \n",
       "3  Hello, I am looking to add voice input capabil...         1       0   \n",
       "4  I recently got a file describing a library of ...         1       0   \n",
       "\n",
       "   medical newsgroup  \n",
       "0        0  graphics  \n",
       "1        0  graphics  \n",
       "2        0  graphics  \n",
       "3        0  graphics  \n",
       "4        0  graphics  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>graphics</th>\n",
       "      <th>hockey</th>\n",
       "      <th>medical</th>\n",
       "      <th>newsgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>carl@SOL1.GPS.CALTECH.EDU (Carl J Lydick) writ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>In article &lt; 1qmlgaINNjab@hp-col.col.hp.com&gt; ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>Article-I.D.: kestrel.1993Apr16.172052.27843 R...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>In article &lt; 1qmlgaINNjab@hp-col.col.hp.com&gt; ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>I have a 42 yr old male friend, misdiagnosed a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXT  graphics  hockey  \\\n",
       "592  carl@SOL1.GPS.CALTECH.EDU (Carl J Lydick) writ...         0       0   \n",
       "593  In article < 1qmlgaINNjab@hp-col.col.hp.com> ,...         0       0   \n",
       "594  Article-I.D.: kestrel.1993Apr16.172052.27843 R...         0       0   \n",
       "595  In article < 1qmlgaINNjab@hp-col.col.hp.com> ,...         0       0   \n",
       "596  I have a 42 yr old male friend, misdiagnosed a...         0       0   \n",
       "\n",
       "     medical newsgroup  \n",
       "592        1   medical  \n",
       "593        1   medical  \n",
       "594        1   medical  \n",
       "595        1   medical  \n",
       "596        1   medical  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TEXT    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[['TEXT']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there are no missing values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If there were missing values:\n",
    "news['TEXT'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assign the input variable to X and the target variable to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news['TEXT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-class classification problem. There are three categories we will predict:<br>\n",
    "Whether a post is \"graphics,\" \"hockey,\" or \"medical\" related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['graphics', 'hockey', 'medical'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = news['newsgroup']\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graphics' 'hockey' 'medical']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "print(le.classes_)\n",
    "y = le.transform(y)\n",
    "\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((417,), (417,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180,), (180,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120    In article 2G1@bcstec.ca.boeing.com, rgc3679@b...\n",
       "58     xgif is the grandfather of XV. -- ============...\n",
       "402    In article < ng4.733990422@husc.harvard.edu> ,...\n",
       "454    Article-I.D.: pitt.19422 References: < 19211@p...\n",
       "314    Article-I.D.: hydra.91678 References: < 1993Ap...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Sklearn: Text preparation\n",
    "\n",
    "For simplicity (and focus), we will not do any text cleaning or preprocessing. We will just use the raw text as input to the model. See the text mining fundamentals tutorial for more details on text cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer includes pre-processing, tokenization, filtering stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', lowercase=True, token_pattern=\"[^\\W\\d_]+\")\n",
    "\n",
    "X_train = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice in the previous step that we use `fit_transform` on TRAIN. When we transform the TEST data, we need to use `transform` only. This enables us to keep the number of columns (features) the same across the data sets. Otherwise, they WILL be different, and no model will work!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the TfidfVectorizer transformation\n",
    "# Be careful: We are using the train fit to transform the test data set. Otherwise, the test data \n",
    "# features will be very different and match the train set!!!\n",
    "\n",
    "X_test = tfidf_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((417, 10192), (180, 10192))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<417x10192 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30482 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These data sets are \"sparse matrix\". We can't see them unless we convert using toarray()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.17269124, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These data sets are \"sparse matrix\". We can't see them unless we convert using toarray()\n",
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Latent Semantic Analysis (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#svd = TruncatedSVD(n_components=300, n_iter=10) #n_components is the number of topics, which should be less than the number of feature\n",
    "    \n",
    "# n_components = 100\n",
    "svd_1 = TruncatedSVD(n_components=100, n_iter=10)\n",
    "X_train_svd_1 = svd_1.fit_transform(X_train)\n",
    "X_test_svd_1 = svd_1.transform(X_test)\n",
    "\n",
    "# n_components = 300\n",
    "svd_2 = TruncatedSVD(n_components=300, n_iter=10)\n",
    "X_train_svd_2 = svd_2.fit_transform(X_train)\n",
    "X_test_svd_2 = svd_2.transform(X_test)\n",
    "\n",
    "# n_components = 500\n",
    "svd_3 = TruncatedSVD(n_components=500, n_iter=10)\n",
    "X_train_svd_3 = svd_3.fit_transform(X_train)\n",
    "X_test_svd_3 = svd_3.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((417, 10192), (180, 10192))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components = 100\n",
      "Train acc: 0.9952\n",
      "Test acc: 0.8833\n",
      "Confusion matrix:\n",
      "[[52  0  9]\n",
      " [ 0 53  5]\n",
      " [ 5  2 54]]\n",
      "\n",
      "n_components = 300\n",
      "Train acc: 0.9952\n",
      "Test acc: 0.9111\n",
      "Confusion matrix:\n",
      "[[53  0  8]\n",
      " [ 0 56  2]\n",
      " [ 3  3 55]]\n",
      "\n",
      "n_components = 500\n",
      "Train acc: 0.9952\n",
      "Test acc: 0.8889\n",
      "Confusion matrix:\n",
      "[[54  0  7]\n",
      " [ 0 53  5]\n",
      " [ 4  4 53]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define n_components to try\n",
    "n_components_list = [100, 300, 500]\n",
    "\n",
    "for n in n_components_list:\n",
    "    print(f\"n_components = {n}\")\n",
    "    \n",
    "    # Apply TruncatedSVD to reduce dimensionality\n",
    "    svd = TruncatedSVD(n_components=n, n_iter=10)\n",
    "    X_train_svd = svd.fit_transform(X_train)\n",
    "    X_test_svd = svd.transform(X_test)\n",
    "    \n",
    "    # Train a Random Forest Classifier on the reduced data\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    _ = rf_clf.fit(X_train_svd, y_train)\n",
    "    \n",
    "    # Evaluate the model on the train set\n",
    "    y_pred_train = rf_clf.predict(X_train_svd)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    print(f\"Train acc: {train_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_test = rf_clf.predict(X_test_svd)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    print(f\"Test acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_test, y_pred_test)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Stochastic Gradient Descent Classifier Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components = 100\n",
      "Train acc (n_components=100): 0.9880\n",
      "Test acc (n_components=100): 0.9611\n",
      "Confusion Matrix (n_components=100): \n",
      "[[59  0  2]\n",
      " [ 1 54  3]\n",
      " [ 1  0 60]]\n",
      "\n",
      "n_components = 300\n",
      "Train acc (n_components=300): 0.9952\n",
      "Test acc (n_components=300): 0.9611\n",
      "Confusion Matrix (n_components=300): \n",
      "[[61  0  0]\n",
      " [ 2 56  0]\n",
      " [ 4  1 56]]\n",
      "\n",
      "n_components = 500\n",
      "Train acc (n_components=500): 0.9952\n",
      "Test acc (n_components=500): 0.9111\n",
      "Confusion Matrix (n_components=500): \n",
      "[[61  0  0]\n",
      " [ 2 56  0]\n",
      " [11  3 47]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in [100, 300, 500]:\n",
    "    print(f\"n_components = {n}\")\n",
    "    svd = TruncatedSVD(n_components=n, n_iter=10)\n",
    "    X_train_svd = svd.fit_transform(X_train)\n",
    "    X_test_svd = svd.transform(X_test)\n",
    "\n",
    "    sgd_clf = SGDClassifier(max_iter=100)\n",
    "    _ = sgd_clf.fit(X_train_svd, y_train)\n",
    "\n",
    "    # Train accuracy\n",
    "    y_pred_train = sgd_clf.predict(X_train_svd)\n",
    "    print(f\"Train acc (n_components={n}): {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "\n",
    "    # Test accuracy\n",
    "    y_pred_test = sgd_clf.predict(X_test_svd)\n",
    "    print(f\"Test acc (n_components={n}): {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(f\"Confusion Matrix (n_components={n}): \\n{confusion_matrix(y_test, y_pred_test)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "1. It is evident from the concept of SVD that this technique reduces the dimensionality of the data.Now that we have used varied n_component values for random forest and stochastic gradient descent models, the accuracy scoring metric is evaluated.\n",
    "\n",
    "\n",
    "2. From the random forest model performance, we observe that training accuracy remained same but the testing accuracy has been decreased when n_component value is changed from 100 to 500.This shows that accuracy gets slightly effected when we use different n_component values.\n",
    "\n",
    "\n",
    "3. From stochastic gradient descent model performance, \n",
    "for n_components(300 and 500), the training accuracy is  0.9976 and both share same values.\n",
    "for all n_Components we see there is high testing accuracy score when n_component value is 500.\n",
    "This shows that Latent Semantic Analyis is a linear combination of orginal values and when n_components score is slightly increased it shows higher accuracy and low variance and I feel it could sometimes lead to overfitting as well.\n",
    "\n",
    "\n",
    "Overall, I have understood from the concept of Principal Component analysis and Latent Semantic Analysis that it is not necessary on datasets which are small but it would be really impact on larger datasets but not datasets like the one used here for the analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
